{
  
    
        "post0": {
            "title": "A comparison of Pets classifier using vanilla PyTorch and Fast.ai",
            "content": "Disclaimer: I am a fairly new to the library and this is just to show what I&#39;ve observed so far, I may be missing a point or two, so, don&#39;t treat the above as a comprehensive list of what fastai can do. This is just my attempt to keep learning and evolving. . Introduction . Below is generally the plan that everyone follows when it comes to training a Machine Learning model: . Load Data | Inspect Data: Plot a few examples | Create a DataLoader | Define a model architecture. | Write a training loop. | Plot metrics. | . There&#39;s also another step, which is . Analyze Errors. | . but we&#39;ll tackle this in a separate blog post once we&#39;ve covered the training of the model bit. . PyTorch Version . from torch import nn from torch import optim from torch.utils.data import dataset, dataloader from torch.autograd import Variable from torch.nn import functional as F from torchvision import datasets, transforms from torchvision.models.resnet import resnet18 from tqdm.notebook import tqdm from sklearn.model_selection import train_test_split import os from collections import Counter, OrderedDict import re import requests import tarfile . . Fetch Data . The data is a tar-gzip archive, to extract data files from it we&#39;ll use the package called tarfile. But first we need to download the archive. . The code in the cell below is taken form: https://gist.github.com/devhero/8ae2229d9ea1a59003ced4587c9cb236#gistcomment-3775721. . def fetch_data(url, data_dir, download=False): if download: response = requests.get(url, stream=True) file = tarfile.open(fileobj=response.raw, mode=&quot;r|gz&quot;) file.extractall(path=data_dir) . In the interest of comparison, I&#39;ll first write the Dataset class and see how easy it gets when we use fastai. The url that we want to fetch the data from is here: Pets Dataset . pets_url = &#39;https://s3.amazonaws.com/fast-ai-imageclas/oxford-iiit-pet.tgz&#39; data_dir = os.path.join(&#39;gdrive&#39;, &#39;MyDrive&#39;, &#39;pets_data&#39;) base_img_dir = os.path.join(data_dir, &#39;oxford-iiit-pet&#39;, &#39;images&#39;) fetch_data(pets_url, data_dir) . . We&#39;ve extracted the data in the folder named pets_data, on inspection, it looks like the folder pets_data/oxford-iiit-pet/images contains all the images we want (some files need to be filtered out as they&#39;re not in JPEG format). The filenames have the category labels in their name itself in the format: &lt;CATEGORYNAME&gt;_&lt;NUMBER&gt;.jpg. . Extract Labels . In order to extract the category name from the file names in fastbook, a RegexLabeller is used. We&#39;ll write a similar LabelExtractor (although it&#39;s very inferior in functionality to the fastai&#39;s RegexLabeller but does the job for now). . class RegexLabelExtractor(): def __init__(self, pattern): self.pattern = pattern self._names = [] def __call__(self, iterable): return [re.findall(self.pattern, value)[0] for value in iterable] . As mentioned before our version, RegexLabelExtractor extracts the label given a text. It accepts a pattern during instantiation and on __call__ it expects an iterable containing a list of texts containing the labels. It returns all the label names in a Python list . Once we&#39;ve defibed a class for extracting the labels, we&#39;d like to define a container that is responsible for maintaining a map of CATEGORYNAME -&gt; ID, which we&#39;ll use to convert the labels to an integer format and vice versa. . Below we define a LabelManager, it exposes a id_for_label* and label_for_id* methods along with keys, which returns the unique label names in our dataset (this our vocabulary size). We can also call len on a LabelManager object to know the number of output classes. . *These are a type of OrderedDict. . class LabelManager(): def __init__(self, labels): self._label_to_idx = OrderedDict() for label in labels: if label not in self._label_to_idx: self._label_to_idx[label] = len(self._label_to_idx) self._idx_to_label = {v:k for k,v in self._label_to_idx.items()} @property def keys(self): return list(self._label_to_idx.keys()) def id_for_label(self, label): return self._label_to_idx[label] def label_for_id(self, idx): return self._idx_to_label[idx] def __len__(self): return len(self._label_to_idx) . Data Splitter . We&#39;d also like to spilt our dataset into train and validation subsets. Although the dataset provides a list of train and validation splits but to be consistent with the book, we&#39;ll just write our version of the RandomSplitter (which again would be very inferior in functionality, but will do the job for the purposes of demonstration). . We&#39;d like this Splitter to accept a percentage to split on and also a seed for reproducibility. . class Splitter(): def __init__(self, valid_pct=0.2, seed = None): self.seed = seed self.valid_pct = valid_pct def __call__(self, dataset): return train_test_split(dataset, test_size=self.valid_pct, random_state=np.random.RandomState(self.seed)) . Writing a PyTorch Dataset . Now that we have a way to extract labels, maintain them in a map and split the data into train and validation splits, we&#39;ll define a PetsDataset ( a PyTorch Dataset) which will be used by the PyTorch DataLoaderto give us the data we need to provide our model to train. . A note on PyTorch Dataset: A PyTorch dataset is a primitive provided by the library that stores the samples and their corresponding labels. In order to write a custom dataset, our class PetsDataset needs to implement three functions: __init__, __len__, and __getitem__. . class PetsDataset(dataset.Dataset): def __init__(self, data, tfms=None): super(PetsDataset, self).__init__() self.data = data self.transforms = tfms def __getitem__(self, idx): X = Image.open(self.data[idx][0]) if X.mode != &#39;RGB&#39;: X = X.convert(&#39;RGB&#39;) y = self.data[idx][1] if self.transforms: X = self.transforms(X) return (X, y) def __len__(self): return len(self.data) . Notice how we&#39;re opening the Image only when __getitem__ is called and we also have to make sure that all the images have 3 input channels, hence the check if X.mode != &#39;RGB&#39;. Some images in the dataset have this issue and if we don&#39;t convert them to have 3 input channels then the DataLoader wouldn&#39;t be able to create a batch using torch.stack . We&#39;re now ready to use these datasets, but we&#39;ll need to make sure that our global map of CATEGORYNAME -&gt; ID is constructed using both the train and the validation splits, we&#39;ll also have this class hold our corresponding datasets. . class DatasetManager(): def __init__(self, base_dir, paths, label_extractor, tfms=None, valid_pct=0.2, seed=None): self._labels = label_extractor(paths) self.tfms = tfms self._label_manager = LabelManager(self._labels) self._label_ids = [self.label_manager.id_for_label(label) for label in self._labels] self.abs_paths = [os.path.join(base_dir, path) for path in paths] self.train_data, self.valid_data = Splitter(valid_pct=valid_pct, seed=seed)(list(zip(self.abs_paths, self._label_ids))) @property def label_manager(self): return self._label_manager @property def train_dataset(self): return PetsDataset(self.train_data, tfms=self.tfms) @property def valid_dataset(self): return PetsDataset(self.valid_data, tfms=self.tfms) . We&#39;ll now use all the helper classes we&#39;ve created so far to use the datasets in a dataloader and look at the plan to choose an architecture and train it (almost there). . paths = [path for path in sorted(os.listdir(base_img_dir)) if path.endswith(&#39;.jpg&#39;)] pattern = &#39;(.+)_ d+.jpg$&#39; regex_label_extractor = RegexLabelExtractor(pattern) dataset_manager = DatasetManager(base_img_dir, paths, regex_label_extractor, tfms=transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()]), seed=42) train_dataset = dataset_manager.train_dataset valid_dataset = dataset_manager.valid_dataset . Before we look at the model, let&#39;s just for sake of sanity look at the labels we&#39;re dealing with and possibly plot a few images. This step is just to make sure that things are working as expected and the dataloader will be batching the data in the right way and our Trainer won&#39;t crash midway. . df = pd.DataFrame(dataset_manager.label_manager.keys, columns=[&#39;label_name&#39;]) df.head(len(df)) . label_name . 0 Abyssinian | . 1 Bengal | . 2 Birman | . 3 Bombay | . 4 British_Shorthair | . 5 Egyptian_Mau | . 6 Maine_Coon | . 7 Persian | . 8 Ragdoll | . 9 Russian_Blue | . 10 Siamese | . 11 Sphynx | . 12 american_bulldog | . 13 american_pit_bull_terrier | . 14 basset_hound | . 15 beagle | . 16 boxer | . 17 chihuahua | . 18 english_cocker_spaniel | . 19 english_setter | . 20 german_shorthaired | . 21 great_pyrenees | . 22 havanese | . 23 japanese_chin | . 24 keeshond | . 25 leonberger | . 26 miniature_pinscher | . 27 newfoundland | . 28 pomeranian | . 29 pug | . 30 saint_bernard | . 31 samoyed | . 32 scottish_terrier | . 33 shiba_inu | . 34 staffordshire_bull_terrier | . 35 wheaten_terrier | . 36 yorkshire_terrier | . . Data Inspection . A method to plot one batch of data (inspired by fastai of course but again a very curtailed version of what that function does). Notice how we&#39;re calling transforms.ToPILImage(), that&#39;s because we have objects of type torch.Tensor in our batch and in order to plot them we need to convert them to a PIL.Image, rest everything is done to just make sure we&#39;ve got the images aligned in a nice way across different panels. . def plot_one_batch(batch, max_images=9): nrows = int(math.sqrt(max_images)) ncols = int(math.sqrt(max_images)) if nrows * ncols != max_images: nrows = (max_images + ncols - 1) // ncols fig, ax = plt.subplots(nrows=nrows, ncols=ncols, figsize=(20, 10)) X,Y = next(batch) for idx, x in enumerate(X[:max_images]): y = Y[idx] ax.ravel()[idx].imshow(transforms.ToPILImage()(x)) ax.ravel()[idx].set_title(f&#39;{y}/{dataset_manager.label_manager.label_for_id(y.item())}&#39;) ax.ravel()[idx].set_axis_off() plt.tight_layout() plt.show() . This function generates one batch of data given a dataloader, this is just using Python Generators . def generate_one_batch(dl): for batch in dl: yield batch . plot_one_batch(generate_one_batch(train_dl), max_images=20) . Model Architecture . Now, we&#39;re ready to look at the model and make a few decisions about the architecture we want to use. . Here&#39;s our requirement: We want to extract the features from an image and then uses a classification head to get the output distribution over the number of classes (our labels from before). We&#39;ll define a loss and use it to optimize the network. . Because we&#39;re dealing with images, a Convolution Neural Network (CNN) seems like a good start, in the literature as well as fastbook, a restnet type architecture has been used, so let&#39;s use that and see what we can do with it. . Coding a ResNet is a separate blog post on its own, so, we&#39;ll punt that for now and use what&#39;s available to us in the form a pretrained model. . model = resnet34(pretrained=True, progress=True) . Downloading: &#34;https://download.pytorch.org/models/resnet34-b627a593.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet34-b627a593.pth . . Changing the classifier . Since this model is trained to give an output distribution for 1000 classes, we can just change that layer to give us an output distribution based on what we have in our dataset and then fine-tune this layer. To read more on fine-tuning refer to fastbook . model.fc = nn.Linear(512,len(dataset_manager.label_manager), bias=True) . Making the model fine-tunable . We&#39;ll freeze all the layers of the model except for the fc classification head we added above. . def make_fine_tunable(model): for param in model.parameters(): param.requires_grad = False for param in model.fc.parameters(): param.requires_grad = True print(&quot;Tunable Layers: &quot;) for (name, param) in model.named_parameters(): if param.requires_grad: print(f&#39;{name} -&gt; {param.requires_grad}&#39;) . make_fine_tunable(model) . Tunable Layers: fc.weight -&gt; True fc.bias -&gt; True . . Trainer . Now comes in a point where we have to write a traininig loop and this is where things get into the Boiler Plate code category even more. We shouldn&#39;t be writing this but that&#39;s the point of this blog post that using fastai we can offload a lot of the boiler plate code to the library and use the goodies offered by the library to our advantage and focus more on research/modeling. . We are maintaining an instance of model, criterion, an optimizer and dataloaders. We step through a batch during train_epoch and incur a loss. We use this loss to make a backward pass and let the optimizer take a step by updating the network parameters. We also have a validate function that calculates loss and accuracy on validation dataset after every epoch . class Trainer(): def __init__(self, train_dataloader, model, criterion, optimizer, test_dataloader=None): self.train_dl = train_dataloader self.model = model self.test_dl = test_dataloader self.criterion = criterion self.optimizer = optimizer self.recorder = {&#39;loss&#39;: { &#39;train&#39;: {}, &#39;test&#39;: {}} , &#39;accuracy&#39;: {&#39;train&#39;: {}, &#39;test&#39;: {}}} def step_batch(self, X,y): X = X.cuda() y = y.cuda() logits = self.model(X) loss = self.criterion(logits, y) probs = F.softmax(logits, dim=1) return loss, logits, probs def train_epoch(self, epoch): self.model.train() running_loss = 0 for X,y in tqdm(self.train_dl, leave=False): self.optimizer.zero_grad() loss, _, _ = self.step_batch(X,y) running_loss += loss loss.backward() self.optimizer.step() epoch_loss = running_loss / len(self.train_dl) self.recorder[&#39;loss&#39;][&#39;train&#39;][epoch] = epoch_loss return epoch_loss @torch.no_grad() def accuracy(self): correct = 0 total = 0 for X,y in tqdm(self.test_dl): total += y.size(0) logits = model(X) probs = F.softmax(logits, dim=1) _, y_pred = torch.max(probs, dim=1) correct += (y_pred == y).sum() acc = correct / float(total) return acc @torch.no_grad() def validate(self, epoch): running_loss = 0 total = 0 correct = 0 for X,y in tqdm(self.test_dl, leave=False): y = y.cuda() total += y.size(0) loss, logits, probs = self.step_batch(X,y) running_loss += loss _, y_pred = torch.max(probs, dim=1) correct += (y_pred == y).cpu().sum() acc = correct / float(total) epoch_loss = running_loss / len(self.test_dl) self.recorder[&#39;loss&#39;][&#39;test&#39;][epoch] = epoch_loss self.recorder[&#39;accuracy&#39;][&#39;test&#39;][epoch] = acc return epoch_loss, acc def train(self, num_epochs): for epoch in tqdm(range(num_epochs), leave=False): train_loss = self.train_epoch(epoch) test_loss, test_acc = self.validate(epoch) #print(f&quot;Training Loss: {train_loss}, tTest Loss: {test_loss}, tTest Accuracy: {test_acc}&quot;) . Training (fine-tuning) the model . Let&#39;s send the model over to the GPU for faster training. . model = model.cuda() . Hyperparameters . Let&#39;s define a configuration that will hold our hyper-parameters . class TrainConfig(): def __init__(self, bs=32, lr=1e-2, seed=42, betas=(0.9, 0.999), num_workers=4): self.bs = bs self.lr = lr self.seed = seed self.betas = betas self.num_workers = num_workers . We set the seed for reproducibility, and instantiate dataloader objects. Notice how we&#39;re using &gt; 1 num_workers. That speeds up the data loading process. . config = TrainConfig(bs=128) torch.manual_seed(config.seed) train_dl = dataloader.DataLoader(train_dataset, batch_size=config.bs, shuffle=True, num_workers=config.num_workers) valid_dl = dataloader.DataLoader(valid_dataset, batch_size=config.bs, shuffle=False, num_workers=config.num_workers) . We define our criterion as nn.CrossEntropy and choose our optimizer to be an instance of optim.Adam, after that we instantiate our trainer object and train (fine-tune in our case) for a few epochs. . Criterion, Optimizer and Training . criterion = nn.CrossEntropyLoss() optimizer = optim.Adam(model.parameters(), lr=1e-3, betas=(0.9, 0.999)) trainer = Trainer(train_dl, model, criterion, optimizer, test_dataloader=valid_dl) trainer.train(10) . Plotting Utilities . Helper functions for plotting our loss and accuracies (which we&#39;ve recorded using our trainer) . def plot_losses(losses): train_loss = losses[&#39;train&#39;] test_loss = losses[&#39;test&#39;] plt.style.use(&#39;fivethirtyeight&#39;) fig, ax = plt.subplots(figsize=(7, 4)) ax.plot(train_loss, color=&#39;blue&#39;, label=&#39;Training Loss&#39;) ax.plot(test_loss, color=&#39;green&#39;, label=&#39;Test Loss&#39;) ax.set(title=&quot;Loss over epochs&quot;, xlabel=&quot;Epochs&quot;, ylabel=&quot;Loss&quot;) ax.legend() fig.show() plt.style.use(&#39;default&#39;) . def plot_accuracy(accuracy): plt.style.use(&#39;fivethirtyeight&#39;) fig, ax = plt.subplots(figsize=(7, 4)) ax.plot(accuracy, color=&#39;blue&#39;, label=&#39;Test Accuracy&#39;) ax.set(title=&quot;Accuracy over epochs&quot;, xlabel=&quot;Epochs&quot;, ylabel=&quot;Accuracy&quot;) ax.legend() fig.show() plt.style.use(&#39;default&#39;) . Loss . losses = { k: np.asarray([t.item() for t in v.values()]) for k,v in trainer.recorder[&#39;loss&#39;].items() } plot_losses(losses) . Accuracy . accuracies = { k: np.asarray([t.item() for t in v.values()]) for k,v in trainer.recorder[&#39;accuracy&#39;].items()} plot_accuracy(accuracy=accuracies[&#39;test&#39;]) . Summary of PyTorch Version . Load Data | Inspect Data: Plot a few examples | Create a DataLoader | Define a model architecture. | Write a training loop. | Plot metrics. | Fast.ai Version . Now let&#39;s see how this can done using fastai. One could treat the following cells as a completely different notebook altogether. . from fastcore.all import L from fastai.vision.all import * matplotlib.rc(&#39;image&#39;, cmap=&#39;Greys&#39;) . . Fetch Data . We&#39;ll first download the Pets data and untar it using the untar_data function and this function really takes care of filtering the images and storing them somewhere on the disk for us and then returning the paths. It&#39;s helpful as I don&#39;t have to take a peek at the response object and parse it then untar it, apply filters and then iterate through the directory. This function does it all for us. To know more about untar_data, please checkout the documentation for untar_data . path = untar_data(URLs.PETS) . Path.BASE_PATH = path path.ls() . (#2) [Path(&#39;annotations&#39;),Path(&#39;images&#39;)] . . (path/&quot;images&quot;).ls() . (#7393) [Path(&#39;images/great_pyrenees_160.jpg&#39;),Path(&#39;images/shiba_inu_82.jpg&#39;),Path(&#39;images/scottish_terrier_2.jpg&#39;),Path(&#39;images/Russian_Blue_144.jpg&#39;),Path(&#39;images/pomeranian_166.jpg&#39;),Path(&#39;images/english_cocker_spaniel_48.jpg&#39;),Path(&#39;images/japanese_chin_180.jpg&#39;),Path(&#39;images/scottish_terrier_15.jpg&#39;),Path(&#39;images/Sphynx_166.jpg&#39;),Path(&#39;images/Maine_Coon_98.jpg&#39;)...] . . Define DataBlock . Let&#39;s construct a DataBlock object. A DataBlock object provides us encapsulation over many aspects of our data loading and arranging pipeline. It let&#39;s us define the . blocks which make up for X and y in our dataset . This will also automtically convert the labels to integer ids | . | extract the label from the name attribute of the file | apply Transformations for us which can help us do Data Augmentation and resizing in one go. | Randomly split the data into training and validation splits. | . Notice how it does all the work and more (we didn&#39;t do any augmentation) of the classes Splitter, RegexLabelExtractor, LabelManager, DatasetManager defined above in just one call, and since it&#39;s well maintained, offers us much more functionality, generic, more performamnt, well tested and maintained, we don&#39;t need to keep writing our own versions from scratch every time we are tasked with training a classifier. . pets = DataBlock(blocks = (ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(seed=42), get_y=using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;), item_tfms=Resize(460), batch_tfms=aug_transforms(size=224, min_scale=0.75)) . To check if everything will work fine, there&#39;s a pretty handy function called summary that we call on the DataBlock object that will show us the whole plan and will tell us a meningful error message if there&#39;s an issue with our pipeline somewhere. . Sanity Check . pets.summary(path/&quot;images&quot;) . Setting-up type transforms pipelines Collecting items from /root/.fastai/data/oxford-iiit-pet/images Found 7390 items 2 datasets of sizes 5912,1478 Setting up Pipeline: PILBase.create Setting up Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} Building one sample Pipeline: PILBase.create starting from /root/.fastai/data/oxford-iiit-pet/images/Persian_180.jpg applying PILBase.create gives PILImage mode=RGB size=334x500 Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} starting from /root/.fastai/data/oxford-iiit-pet/images/Persian_180.jpg applying partial gives Persian applying Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} gives TensorCategory(7) Final sample: (PILImage mode=RGB size=334x500, TensorCategory(7)) Collecting items from /root/.fastai/data/oxford-iiit-pet/images Found 7390 items 2 datasets of sizes 5912,1478 Setting up Pipeline: PILBase.create Setting up Pipeline: partial -&gt; Categorize -- {&#39;vocab&#39;: None, &#39;sort&#39;: True, &#39;add_na&#39;: False} Setting up after_item: Pipeline: Resize -- {&#39;size&#39;: (460, 460), &#39;method&#39;: &#39;crop&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;resamples&#39;: (2, 0), &#39;p&#39;: 1.0} -&gt; ToTensor Setting up before_batch: Pipeline: Setting up after_batch: Pipeline: IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} -&gt; Flip -- {&#39;size&#39;: None, &#39;mode&#39;: &#39;bilinear&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;mode_mask&#39;: &#39;nearest&#39;, &#39;align_corners&#39;: True, &#39;p&#39;: 0.5} -&gt; RandomResizedCropGPU -- {&#39;size&#39;: (224, 224), &#39;min_scale&#39;: 0.75, &#39;ratio&#39;: (1, 1), &#39;mode&#39;: &#39;bilinear&#39;, &#39;valid_scale&#39;: 1.0, &#39;max_scale&#39;: 1.0, &#39;p&#39;: 1.0} -&gt; Brightness -- {&#39;max_lighting&#39;: 0.2, &#39;p&#39;: 1.0, &#39;draw&#39;: None, &#39;batch&#39;: False} Building one batch Applying item_tfms to the first sample: Pipeline: Resize -- {&#39;size&#39;: (460, 460), &#39;method&#39;: &#39;crop&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;resamples&#39;: (2, 0), &#39;p&#39;: 1.0} -&gt; ToTensor starting from (PILImage mode=RGB size=334x500, TensorCategory(7)) applying Resize -- {&#39;size&#39;: (460, 460), &#39;method&#39;: &#39;crop&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;resamples&#39;: (2, 0), &#39;p&#39;: 1.0} gives (PILImage mode=RGB size=460x460, TensorCategory(7)) applying ToTensor gives (TensorImage of size 3x460x460, TensorCategory(7)) Adding the next 3 samples No before_batch transform to apply Collating items in a batch Applying batch_tfms to the batch built Pipeline: IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} -&gt; Flip -- {&#39;size&#39;: None, &#39;mode&#39;: &#39;bilinear&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;mode_mask&#39;: &#39;nearest&#39;, &#39;align_corners&#39;: True, &#39;p&#39;: 0.5} -&gt; RandomResizedCropGPU -- {&#39;size&#39;: (224, 224), &#39;min_scale&#39;: 0.75, &#39;ratio&#39;: (1, 1), &#39;mode&#39;: &#39;bilinear&#39;, &#39;valid_scale&#39;: 1.0, &#39;max_scale&#39;: 1.0, &#39;p&#39;: 1.0} -&gt; Brightness -- {&#39;max_lighting&#39;: 0.2, &#39;p&#39;: 1.0, &#39;draw&#39;: None, &#39;batch&#39;: False} starting from (TensorImage of size 4x3x460x460, TensorCategory([ 7, 22, 6, 14], device=&#39;cuda:0&#39;)) applying IntToFloatTensor -- {&#39;div&#39;: 255.0, &#39;div_mask&#39;: 1} gives (TensorImage of size 4x3x460x460, TensorCategory([ 7, 22, 6, 14], device=&#39;cuda:0&#39;)) applying Flip -- {&#39;size&#39;: None, &#39;mode&#39;: &#39;bilinear&#39;, &#39;pad_mode&#39;: &#39;reflection&#39;, &#39;mode_mask&#39;: &#39;nearest&#39;, &#39;align_corners&#39;: True, &#39;p&#39;: 0.5} gives (TensorImage of size 4x3x460x460, TensorCategory([ 7, 22, 6, 14], device=&#39;cuda:0&#39;)) applying RandomResizedCropGPU -- {&#39;size&#39;: (224, 224), &#39;min_scale&#39;: 0.75, &#39;ratio&#39;: (1, 1), &#39;mode&#39;: &#39;bilinear&#39;, &#39;valid_scale&#39;: 1.0, &#39;max_scale&#39;: 1.0, &#39;p&#39;: 1.0} gives (TensorImage of size 4x3x224x224, TensorCategory([ 7, 22, 6, 14], device=&#39;cuda:0&#39;)) applying Brightness -- {&#39;max_lighting&#39;: 0.2, &#39;p&#39;: 1.0, &#39;draw&#39;: None, &#39;batch&#39;: False} gives (TensorImage of size 4x3x224x224, TensorCategory([ 7, 22, 6, 14], device=&#39;cuda:0&#39;)) . . Dataloader . Let&#39;s define our dataloader. . dls = pets.dataloaders(path/&quot;images&quot;) . Training (fine-tuning) the model . Let&#39;s train our model for two epochs using cnn_learner, the model we&#39;ll use is resnet34 . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(10) . epoch train_loss valid_loss error_rate time . 0 | 1.492460 | 0.365994 | 0.117727 | 01:09 | . epoch train_loss valid_loss error_rate time . 0 | 0.497617 | 0.326611 | 0.109608 | 01:14 | . 1 | 0.303163 | 0.237988 | 0.077131 | 01:14 | . Notice how we didn&#39;t have to worry about sending the data or the model over to the GPU. . Interpretation and Analysis . And the cherry on top is the ability to do interpretation and analyze errors with a very neatly written function call. . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix(figsize=(12,12), dpi=60) . . Summary . And we&#39;re done! Sure the model can be improved upon from here, but the point is that I can now focus on that bit precisely after just getting started and not worry about anything else. I&#39;d advice now to please read the chapter 5 of the fastbook as the last few lines have missed a few points about Data Augmentation, finding the right Learning Rate etc. . References . Fastai docs | Fastbook Chapter 5 | Weights &amp; Biases forum for chapter 5 | Implementing Yann LeCun’s LeNet-5 in PyTorch | How can I disable all layers gradient expect the last layer in Pytorch? | Grayscale to RGB transform | PyTorch PIL to Tensor and vice versa | Deep residual Learning for Image Recognition | .",
            "url": "https://akashmehra.github.io/fastpages/fastpages/jupyter/image_classification/fastai/pytorch/pets/fastbook/fine_tuning/2021/07/20/ResNet34_based_pets_classifier_using_PyTorch_and_Fast_ai.html",
            "relUrl": "/fastpages/jupyter/image_classification/fastai/pytorch/pets/fastbook/fine_tuning/2021/07/20/ResNet34_based_pets_classifier_using_PyTorch_and_Fast_ai.html",
            "date": " • Jul 20, 2021"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I am Akash Mehra, a Machine Learning Engineer. This is my personal blog* where I write about my learnings in the field of Machine Learning and Artifical Intelligence. I am very passionate about math and its applications and usually find myself reading math theory in my free time. I love playing football and cricket when I am not working, reading, sleeping or working out. . [^*] The views expressed here are mine alone and not those of my employer. .",
          "url": "https://akashmehra.github.io/fastpages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://akashmehra.github.io/fastpages/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}