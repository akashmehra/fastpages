<h2 id="optimization-parameter-updates-and-hyper-parameters">Optimization, Parameter Updates and Hyper-parameters</h2>

<h3 id="gradient-based-optimization">Gradient based optimization</h3>

<p>Gradient based optimization is a technique to minimize/maximize the function by updating the paremeters/weights of a model using the gradients of the Loss wrt to the parameters. If the Loss function is denoted by $\mathcal{L}(w)$​​​, where $w$​​​ are the parameters then we’d like to calculate $\nabla_{w} \mathcal{L}(w)$​​​ and to get the parameters for the next iteration, we’d like to perform the  $w_{t+1} = w_{t} - \alpha * \nabla_{w} \mathcal{L}(w_{t})$, where  $\alpha$​​​​ is the learning rate.</p>

<blockquote>
  <p>In order to update the parameters of a Machine Learning model we need a way for us to measure the rate of change in the output when the inputs (the parameters) are changed.</p>
</blockquote>

<p>Here, we’ve assumed that the function $\mathcal{L}(w)$​​ is continuous. The function $\mathcal{L}(w)$​ can have kinks in which case we’ll call the <em>gradient</em> a <em>subgradient</em>. <em>Subgradient</em> generalizes the notion of a derivative to functions that are not necessirily differentiable. More on <em>subgradients</em> will follow in a separate post, but for now assume that there exists a concept using which one can calculate the <em>gradient</em> of a function that is not differentiable everywhere (has kinks, for example: <code class="language-plaintext highlighter-rouge">ReLU</code> non-linearity).</p>

<p>There are many gradient based optimization algorithms that exist and they differ mainly in how the gradients are calculated or how the learning rate $\alpha$​ is chosen. We’ll look at some of those algorithms that are used in practice.</p>

<h3 id="optimization-basics">Optimization basics</h3>

<h4 id="univariate-optimality-conditions-quick-recap">Univariate Optimality Conditions: Quick recap</h4>

<p>Consider a function $f(x)$​, where $x$​ is univariate, the necessary conditions for a point $x=x_0$​ to be a minimum  of $f(x)$​  with respect to its <em>infinitesimal locality</em> are: $f’(x_0) = 0$​ and $f’‘(x_0) &gt; 0$​. The optimality conditions can be well undertsood if we look at the <em>Taylor Series</em> expansion of $f(x)$​ in the small vicinty of $x_0 + \Delta$​.
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo>+</mo><mi mathvariant="normal">Δ</mi><mo stretchy="false">)</mo><mo>≈</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="false">)</mo><mo>+</mo><mi mathvariant="normal">Δ</mi><msup><mi>f</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><mfrac><msup><mi mathvariant="normal">Δ</mi><mn>2</mn></msup><mn>2</mn></mfrac><msup><mi>f</mi><mrow><mo mathvariant="normal">′</mo><mo mathvariant="normal">′</mo></mrow></msup><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x_0 + \Delta) \approx f(x_0) + \Delta f&#x27;(x) + \frac{\Delta^2}{2} f&#x27;&#x27;(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">Δ</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.001892em;vertical-align:-0.25em;"></span><span class="mord">Δ</span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.36292em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01792em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mtight">Δ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913142857142857em;"><span style="top:-2.931em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathdefault">x</span><span class="mclose">)</span></span></span></span>
Here, the value of $\Delta$​ is assumed to be very small. One can see that if $f’(x) = 0$​ and $f’‘(x) &gt; 0$​ then $f(x_0 + \Delta) \approx f(x_0) + \epsilon$​​, which means that $f(x_0) &lt; f(x_0 + \Delta)$​, for small values of $\Delta$​ (whether it is positive or negative) or $x_0$​ is a minimum wrt to its immediate locality.</p>

<h4 id="multivariate-optimality-conditions">Multivariate Optimality Conditions</h4>

<p>Consider a function $f(x)$ where $x$ is an n-dimensional vector given by $\begin {bmatrix}x_1, x_2, x_3, \cdots, x_n \end {bmatrix}^T$. The <em>gradient vector</em> of $f(x)$ is</p>

<p>given by the partial derivatives wrt to each of the components of $x$, $\nabla {f{x}} \equiv g(x) \equiv \begin {bmatrix} 
\frac{\partial f}{\partial x_1}<br />
\frac{\partial f}{\partial x_2}<br />
\frac{\partial f}{\partial x_3}<br />
\vdots <br />
\frac{\partial f}{\partial x_n}<br />
\end{bmatrix}$</p>

<p>Note that the <em>gradient</em> in case of a <em>multivariate</em> functions is vector of <em>n-dimensions</em>. Similarly, one can define the <em>second derivative</em> of a <em>multivariate</em> function using a matrix of size $n \cross n$
$$
\nabla^2f(x)\equiv H(x) \equiv \begin{bmatrix}
\frac{\partial^2 f}{\partial x_{1}^2} &amp;  \cdots &amp; \frac{\partial^2 f}{\partial x_{1}\partial x_{n}}<br />
\vdots &amp; &amp; \vdots <br />
\frac{\partial^2 f}{\partial x_{n}\partial x_{1}} &amp; \cdots &amp;  \frac{\partial^2 f}{\partial x_{n}^2}\</p>

<p>\end{bmatrix}
$$
Here, $H(x)$​​​​ is called a <em>Hessian Matrix</em>, and if the partial derivatives ${\partial^2 f}/{\partial x_{i}\partial x_{j}}$​​​​​ and ${\partial^2 f}/{\partial x_{j} \partial x_{i}}$​​​​​  are both defined and continuous and then by <em>Clairaut’s Theorem</em> $\partial^2 f/\partial x_{i}\partial x_{j}$​​​​​​​ = $\partial^2 f/\partial x_{j}\partial x_{i}$​​​​​,​​ this second order <em>partial derivative</em> matrix becomes symmetric.</p>

<p>If $f$ is quadratic then the <em>Hessian</em> becomes a constant, the function can then be expressed as: $f(x) = \frac{1}{2}x^THx + g^Tx + \alpha$ , and as in case of <em>univariate</em> case the optimality conditions can be derived by looking at the <em>Taylor Series</em> expansion of $f$ about $x_0$: 
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo>+</mo><mi>ϵ</mi><mover accent="true"><mi>v</mi><mo stretchy="true">‾</mo></mover><mo stretchy="false">)</mo><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="false">)</mo><mo>+</mo><mi>ϵ</mi><msup><mover accent="true"><mi>v</mi><mo stretchy="true">‾</mo></mover><mi>T</mi></msup><mi mathvariant="normal">∇</mi><mi>f</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo stretchy="false">)</mo><mo>+</mo><mfrac><msup><mi>ϵ</mi><mn>2</mn></msup><mn>2</mn></mfrac><msup><mover accent="true"><mi>v</mi><mo stretchy="true">‾</mo></mover><mi>T</mi></msup><mi>H</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mn>0</mn></msub><mo>+</mo><mi>ϵ</mi><mi>θ</mi><mover accent="true"><mi>v</mi><mo stretchy="true">‾</mo></mover><mo stretchy="false">)</mo><mover accent="true"><mi>v</mi><mo stretchy="true">‾</mo></mover></mrow><annotation encoding="application/x-tex">f(x_0 + \epsilon\overline{v}) = f(x_0) + \epsilon \overline{v}^T \nabla f(x_0) + \frac {\epsilon^2}{2} \overline{v}^T H(x_0 + \epsilon \theta \overline{v}) \overline{v}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">ϵ</span><span class="mord overline"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.63056em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">v</span></span></span><span style="top:-3.55056em;"><span class="pstrut" style="height:3em;"></span><span class="overline-line" style="border-bottom-width:0.04em;"></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.111791em;vertical-align:-0.25em;"></span><span class="mord mathdefault">ϵ</span><span class="mord"><span class="mord overline"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.63056em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">v</span></span></span><span style="top:-3.55056em;"><span class="pstrut" style="height:3em;"></span><span class="overline-line" style="border-bottom-width:0.04em;"></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.861791em;"><span style="top:-3.08346em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord">∇</span><span class="mord mathdefault" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1.36292em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01792em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathdefault mtight">ϵ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8913142857142857em;"><span style="top:-2.931em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord"><span class="mord overline"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.63056em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">v</span></span></span><span style="top:-3.55056em;"><span class="pstrut" style="height:3em;"></span><span class="overline-line" style="border-bottom-width:0.04em;"></span></span></span></span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.861791em;"><span style="top:-3.08346em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">T</span></span></span></span></span></span></span></span><span class="mord mathdefault" style="margin-right:0.08125em;">H</span><span class="mopen">(</span><span class="mord"><span class="mord mathdefault">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">ϵ</span><span class="mord mathdefault" style="margin-right:0.02778em;">θ</span><span class="mord overline"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.63056em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">v</span></span></span><span style="top:-3.55056em;"><span class="pstrut" style="height:3em;"></span><span class="overline-line" style="border-bottom-width:0.04em;"></span></span></span></span></span></span><span class="mclose">)</span><span class="mord overline"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.63056em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.03588em;">v</span></span></span><span style="top:-3.55056em;"><span class="pstrut" style="height:3em;"></span><span class="overline-line" style="border-bottom-width:0.04em;"></span></span></span></span></span></span></span></span></span>
where $0 \leq \theta \leq 1$​, $\epsilon$​ is a <em>scalar</em> and $\overline{v}$​ is an <em>n-dimensional</em> vector. Now, if $\nabla f(x_0) = 0$​, then it leaves us with $f(x_0) + \frac{\epsilon^2}{2} \overline{v}^T H \overline{v}$​, which implies that for the $x_0$​ to be a point of minima,  $\overline{v}^T H \overline{v}&gt; 0$​​ or the <em>Hessian</em> has to be <em>positive definite</em>.</p>

<blockquote>
  <p>Quick note on definiteness of the symmetric Hessian Matrix</p>
</blockquote>

<ul>
  <li>$H$​ is <em>positive definite</em> if $\bold v^TH \bold v &gt; 0$​, for all <em>non-zero vectors</em> $\bold v \in \mathbb{R}^n$​​ (all <em>eigenvalues</em> of $H$​ are <em>strictly positive</em>)</li>
  <li>$H$​ is <em>positive semi-definite</em> if $\bold v^TH \bold v \geq 0$​, for all <em>non-zero vectors</em> $\bold v \in \mathbb{R}^n$​ (<em>eigenvalues</em> of $H$​ are <em>positive</em> or <em>zero</em>)</li>
  <li>$H$​​ is <em>indefinite</em> if there exists a  $\bold v, \bold u \in \mathbb{R}^n$​, such that $\bold v^TH \bold v &gt; 0$ and $\bold u^T H \bold u &lt; 0$ (<em>eigenvalues</em> of $H$ have mixed sign)</li>
  <li>$H$​​ is <em>negative definite</em> if $\bold v^TH \bold v &lt; 0$​​, for all <em>non-zero vectors</em> $\bold v \in \mathbb{R}^n$​​ (all <em>eigenvalues</em> of $H$​​ are <em>strictly negative</em>)</li>
</ul>

<h3 id="need-for-gradient-descent">Need for Gradient Descent</h3>

<p>One of the necessary conditions for a point to be a critical point (minima, maxima or saddle) is that the first order derivate $f’(x) = 0$, it is often the case that we’re not able to exactly solve this equation because the derivative can be a complex function of $x$. A <em>closed form solution</em> so to speak, doesn’t exist and things get even more complicated in <em>multivariate</em> case due to compultational and numerical challenges<sup>[1]</sup>. We use <em>Gradient Descent</em> to iteratively solve the optimization problem irrespective of the functional form of $f(x$) by taking a step in the direction of the steepest descent (because in Machine Learning we’re optimizing a <em>Loss</em> or a <em>Cost</em> function, we tend to always solve the optimization problem from the perspective of <em>minimization</em>).</p>

